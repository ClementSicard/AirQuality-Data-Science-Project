{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0477d2b383c49e1ec6e86d0e875214c11d49545137877c5c3b99ad7c21c9bd5e4",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thingspeak\n",
    "import pandas as pd\n",
    "import json\n",
    "import ssl\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import seaborn as sn\n",
    "from typing import Tuple\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "\n",
    "# Configuring Matplotlib\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "savefig_options = dict(format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"ZRDS32VNQEEFSOF4\"\n",
    "CHANNEL_ID = \"1361623\"\n",
    "\n",
    "fields = {\n",
    "    \"field2\": \"TEMP [C]\", \n",
    "    \"field3\":\"Relative humidity\", \n",
    "    \"field4\":\"PM1 [ug/m3]\", \n",
    "    \"field5\":\"PM2.5 [ug/m3]\", \n",
    "    \"field6\":\"PM10 [ug/m3]\",\n",
    "    \"created_at\" : \"Date/time\"\n",
    "}\n",
    "\n",
    "new_fields = {v : k for (k,v) in zip(fields.keys(), fields.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_plotly():\n",
    "    pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "setup_plotly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thingspeak_data(id: str, api_key: str, nb_of_results: int = 8000, drop_entry_id: bool = True, save: bool = False) -> pd.DataFrame:\n",
    "    time_zone = \"UTC\"\n",
    "    #url = f\"https://thingspeak.com/channels/{id}/feed.csv?apikey={api_key}&results={nb_of_results}&timezone={time_zone}\"\n",
    "    \n",
    "    #df = pd.read_csv(url)\n",
    "    df = pd.read_csv('data/thingspeak_data.csv')\n",
    "\n",
    "    df.replace(['None'], np.nan, inplace=True)\n",
    "    df.rename(columns=fields, inplace=True)\n",
    "    df[\"Date/time\"] = pd.to_datetime(df[\"Date/time\"])\n",
    "    df[\"TEMP [C]\"] = df[\"TEMP [C]\"].astype(\"float32\")\n",
    "    df.set_index(\"Date/time\", inplace=True)\n",
    "    df.index = df.index.tz_convert(\"Europe/Paris\")\n",
    "    if save:\n",
    "        df.to_csv('data/thingspeak_data.csv')\n",
    "    df = df.assign(missing= np.nan)\n",
    "    df.drop(\"missing\", inplace=True, axis=1)\n",
    "    df.drop(\"Relative humidity\", axis=1, inplace=True)\n",
    "    df.drop(\"PM1 [ug/m3]\", axis=1, inplace=True)\n",
    "    \n",
    "    if drop_entry_id:\n",
    "        df.drop('entry_id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zue_data():\n",
    "    df = pd.read_csv('data/ZUE.csv', delimiter=\";\")\n",
    "    df[\"Date/time\"] = pd.to_datetime(df[\"Date/time\"], format='%d.%m.%Y %H:%M')\n",
    "    df[\"TEMP [C]\"] = df[\"TEMP [C]\"].astype(\"float32\")\n",
    "    df.set_index(\"Date/time\", inplace=True)\n",
    "    df.index = df.index.tz_localize(\"Europe/Paris\", ambiguous=\"NaT\", nonexistent=\"shift_backward\")\n",
    "    df = df.assign(missing= np.nan)\n",
    "    df.drop(\"missing\", inplace=True, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_series(df: pd.DataFrame):\n",
    "    pm10_series = df.loc[:, \"PM10 [ug/m3]\"]\n",
    "    pm25_series = df.loc[:, \"PM2.5 [ug/m3]\"]\n",
    "    temperature_series = df.loc[:, \"TEMP [C]\"]\n",
    "    precipitation_series = df.loc[:, \"PREC [mm]\"]\n",
    "\n",
    "    return temperature_series, precipitation_series, pm25_series, pm10_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we use a sliding window with std to detect and remove outliers\n",
    "def remove_outliers(df, param=1.6, hours=0, minutes=0):\n",
    "    for t in df.index:\n",
    "        t1 = t - timedelta(hours=hours, minutes=minutes)\n",
    "        t2 = t + timedelta(hours=hours, minutes=minutes)\n",
    "        inter = df.loc[t1:t2]\n",
    "        mean = inter.mean(axis=0)\n",
    "        std = inter.std(axis=0)\n",
    "        dist = np.abs(df.loc[t] - mean)\n",
    "        df.loc[t] = df.loc[t].where(dist < param * std)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wanted to interpolate the large missing intervals with DTW but the results were inconclusive\n",
    "# due to the high correlation between pm10 and pm25 (~0.95) we decided to interpolate with a simple multiplication\n",
    "def interpolate_pm10(df):\n",
    "    pm10_series = df[\"PM10 [ug/m3]\"]\n",
    "    pm25_series = df[\"PM2.5 [ug/m3]\"]\n",
    "\n",
    "    mean_factor = (pm10_series/pm25_series).mean(axis=0)\n",
    "\n",
    "    # interpolate with multiplication of pm2.5\n",
    "    pm10_na = pm10_series.isna()\n",
    "    pm10_series[pm10_na] = mean_factor * pm25_series[pm10_na]\n",
    "\n",
    "    df[\"PM10 [ug/m3]\"] =  pm10_series\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_datasets(df_zue: pd.DataFrame, df_ts: pd.DataFrame, save_data=False) -> pd.DataFrame:\n",
    "\n",
    "    # time interpolation corresponds to a linear interpolation but \n",
    "    # it takes into account the time distance between each index when\n",
    "    # assigning a value\n",
    "\n",
    "    # except for the case of pm10 we decided to use time interpolation for \n",
    "    # scattered missing values\n",
    "\n",
    "    # reorder the columns according to zue data\n",
    "    # crop the ts data that has no corresponding zue data\n",
    "    # resample the ts data to have exactly 5 minutes indexes\n",
    "    ts = df_ts.copy()\n",
    "    ts = ts[ts.index.notnull()]\n",
    "    ts = ts[df_zue.columns[:3]]\n",
    "    ts = ts.loc[:df_zue.index[-1]] \n",
    "    ts = ts.resample('5min').mean()\n",
    "    start = ts.index[0]\n",
    "\n",
    "    if save_data : ts.to_csv('data_v/ts.csv')\n",
    " \n",
    "    # crop zue data and save precipitations separately\n",
    "    zue = df_zue.copy()\n",
    "    zue = zue[zue.index.notnull()]\n",
    "    zue = zue.loc[:ts.index[-1]]\n",
    "    precipitations = zue[\"PREC [mm]\"]\n",
    "    zue.drop(\"PREC [mm]\", axis=1, inplace=True)\n",
    "\n",
    "    if save_data : zue.to_csv('data_v/zue.csv')\n",
    "\n",
    "    # interpolate the pm10 zue data for which we have large missing intervals\n",
    "    zue = interpolate_pm10(zue)\n",
    "\n",
    "    if save_data : zue.to_csv('data_v/zue_pm10_inter.csv')\n",
    "\n",
    "    # remove the outliers and interpolate their value with time\n",
    "    zue = remove_outliers(zue, hours=5)\n",
    "    ts = remove_outliers(ts, minutes=30)\n",
    "\n",
    "    zue = zue.interpolate(method='time')\n",
    "    ts = ts.interpolate(method='time')\n",
    "\n",
    "    if save_data : zue.to_csv('data_v/zue_no_outliers.csv')\n",
    "    if save_data : ts.to_csv('data_v/ts_no_outliers.csv')\n",
    "   \n",
    "    # upsample zue data to 5mn and interpolate with time\n",
    "    # separate between previous and overlapping data\n",
    "    zue = zue.resample('5min').interpolate('time')\n",
    "    zue_prev = zue.loc[:start].drop(start)\n",
    "    zue_superpos = zue.loc[start:ts.index[-1]]\n",
    "\n",
    "    if save_data : zue_superpos.to_csv('data_v/zue_superpos.csv')\n",
    "\n",
    "    # scale our ts data according to the zue data\n",
    "    h_mean_ts = pd.DataFrame().reindex_like(ts)\n",
    "    for t in ts.index:\n",
    "        t1 = t - timedelta(minutes=30)\n",
    "        t2 = t + timedelta(minutes=30)\n",
    "        h_mean_ts.loc[t] = ts.loc[t1:t2].mean(axis=0)\n",
    "\n",
    "    if save_data : h_mean_ts.to_csv('data_v/h_mean_ts.csv')\n",
    "\n",
    "    scaled_ts = zue_superpos * ts / h_mean_ts\n",
    "\n",
    "    if save_data : scaled_ts.to_csv('data_v/scaled_ts.csv')\n",
    "    \n",
    "    # calculate our scaled ts data variation around the zue data (proportianl to the zue values)\n",
    "    dist_ts = scaled_ts - zue_superpos\n",
    "    dist_ts = (scaled_ts - zue_superpos) / zue_superpos\n",
    "\n",
    "    h = dist_ts.hist()\n",
    "    h.show()\n",
    "    \n",
    "    # calculate our variation properties (assumed normal from the histogram)\n",
    "    means = dist_ts.mean(axis = 0)\n",
    "    covs = dist_ts.cov()\n",
    "\n",
    "    # add noise to the zue data with a distribution corresponding to the ts data and proportionate to \n",
    "    # the zue values\n",
    "    zue_prev += zue_prev * np.random.multivariate_normal(means, covs, zue_prev.shape[0])\n",
    "\n",
    "    # create the result dataset by concatenating the noisy previous zue data with\n",
    "    # the scaled ts data, adding back precipitations, interpolating any possibly missing values and \n",
    "    # setting the minimal value for the particule measures to 0\n",
    "    result = pd.concat([zue_prev, scaled_ts])\n",
    "    result[\"PREC [mm]\"] = precipitations\n",
    "    result = result.interpolate('time')\n",
    "    result[[\"PM10 [ug/m3]\", \"PM2.5 [ug/m3]\"]] = result[[\"PM10 [ug/m3]\", \"PM2.5 [ug/m3]\"]].clip(lower=0)\n",
    "\n",
    "    if save_data : result.to_csv('data_v/dataset.csv')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(save: bool = True) -> pd.DataFrame: \n",
    "    df_zue = get_zue_data()\n",
    "    df_ts = get_thingspeak_data(id=CHANNEL_ID, api_key=API_KEY, save=True) \n",
    "    df = merge_datasets(df_ts=df_ts, df_zue=df_zue)\n",
    "    if save :\n",
    "        result.to_csv('data/dataset.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = get_dataset(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data_v/dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_series, precipitation_series, pm25_series, pm10_series = extract_time_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dfplot = pd.read_csv('data_v/dataset.csv')\n",
    "dfplot.set_index(\"Date/time\", inplace=True)\n",
    "dfplot.plot(title=\"Comparison graph\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data_v/ts_no_outliers.csv')\n",
    "df1 = df1.set_index(\"Date/time\")\n",
    "df1.columns = df1.columns + ' PI'\n",
    "\n",
    "df2 = pd.read_csv('data_v/zue_superpos.csv')\n",
    "df2 = df2.set_index(\"Date/time\")\n",
    "df2.columns = df2.columns + ' ZUE'\n",
    "\n",
    "df4 = pd.read_csv('data_v/scaled_ts.csv')\n",
    "df4 = df4.set_index(\"Date/time\")\n",
    "df4.columns = df4.columns + ' Scaled PI'\n",
    "\n",
    "dfplot = df1.append(df2).append(df4)\n",
    "#dfplot = dfplot[[\"PM2.5 [ug/m3] PI\", \"PM2.5 [ug/m3] ZUE\", \"PM2.5 [ug/m3] Scaled PI\"]]\n",
    "dfplot = dfplot[[\"TEMP [C] PI\", \"TEMP [C] ZUE\", \"TEMP [C] Scaled PI\"]]\n",
    "\n",
    "dfplot.plot(title=\"Comparison graph\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(title='Dataset')\n",
    "fig.update_layout(width=1100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = df.corr()\n",
    "px.imshow(corrMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sn.heatmap(corrMatrix, annot=True)\n",
    "plt.get_figure().savefig('plots/correlogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_series.plot(title=\"Precipitation time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_series.plot(title=\"Temperature time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_series.plot(title=\"PM10 time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_series.plot(title=\"PM2.5 time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_series.interpolate('linear').plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as pl\n",
    "from numpy import fft\n",
    "\n",
    "# we try to find patterns in our data using the Fourier transform,\n",
    "# replicating it and testing its loss against our true data\n",
    "# the results are quite bad, we could not find meaningful pattern at\n",
    "# this scale (but it might have patterns at smaller/bigger scales)\n",
    "\n",
    "def plot(t1, t2, tp, sigma=0):\n",
    "    pl.plot(range(0,len(t1)), gaussian_filter(t1, sigma=sigma), 'b', label='x_train_val', linewidth=1)\n",
    "    pl.plot(range(len(t1),len(tp)), gaussian_filter(t2, sigma=sigma), 'g', label='x_test', linewidth=1)\n",
    "    pl.plot(range(0, len(tp)), tp, 'r', label = 'extrapolation')\n",
    "    pl.legend\n",
    "    pl.show()\n",
    "\n",
    "def mape(actual, pred): \n",
    "    actual, pred = np.array(actual) + 1, np.array(pred) + 1\n",
    "    return np.mean(np.abs((actual - pred) / actual)) * 100\n",
    "\n",
    "def mae(actual, pred):\n",
    "    return np.mean(np.abs(actual - pred))\n",
    "    \n",
    "def fourierExtrapolation(x, n_predict, blur):\n",
    "    n = x.size\n",
    "    n_harm = round(len(x)/blur)      # number of harmonics in model\n",
    "    t = np.arange(0, n)\n",
    "    p = np.polyfit(t, x, 1)         # find linear trend in x\n",
    "    x_notrend = x - p[0] * t        # detrended x\n",
    "    x_freqdom = fft.fft(x_notrend)  # detrended x in frequency domain\n",
    "    f = fft.fftfreq(n)              # frequencies\n",
    "    indexes = list(range(n))\n",
    "    # sort indexes by frequency, lower -> higher\n",
    "    indexes.sort(key = lambda i: np.absolute(f[i]))\n",
    " \n",
    "    t = np.arange(0, n + n_predict)\n",
    "    restored_sig = np.zeros(t.size)\n",
    "    for i in indexes[:1 + n_harm * 2]:\n",
    "        ampli = np.absolute(x_freqdom[i]) / n   # amplitude\n",
    "        phase = np.angle(x_freqdom[i])          # phase\n",
    "        restored_sig += ampli * np.cos(2 * np.pi * f[i] * t + phase)\n",
    "    return restored_sig + p[0] * t\n",
    "    \n",
    "def main(x, percent, blur=500, plot_data=False):\n",
    "    length = len(x)\n",
    "    val_cut = round(length * percent)\n",
    "    x_train = x[:val_cut]\n",
    "    x_val = x[val_cut:]\n",
    "\n",
    "    n_predict = length - val_cut\n",
    "    extrapolation = fourierExtrapolation(x_train, n_predict, blur)\n",
    "\n",
    "    #plot our data with a gaussian blur applied\n",
    "    if plot_data : plot(x_train, x_val, extrapolation, sigma=50)\n",
    "\n",
    "    return mae(x_train, extrapolation[:val_cut]), mae(x_val, extrapolation[val_cut:])\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    # we use the last 20% of our data for testing because we are\n",
    "    # looking for long time repeating patterns and as such prefer a\n",
    "    # important testing set\n",
    "    x = pm25_series.to_numpy()\n",
    "    length = len(x)\n",
    "    test_cut = round(length * 0.8)\n",
    "    x_train_val = x[:test_cut]\n",
    "    x_test = x[test_cut:]\n",
    "\n",
    "    # we explore a 2D grid of parameters for the best\n",
    "    # matching ones and save them \n",
    "    train_loss = []\n",
    "    val_loss = []\n",
    "    min_loss = np.inf\n",
    "    best_params = None\n",
    "    \n",
    "    for p in range(20, 85, 5):\n",
    "        for b in range(500, 5000, 50):\n",
    "            tl, vl = main(x_train_val, p/100, b)\n",
    "            train_loss.append(tl)\n",
    "            val_loss.append(vl)\n",
    "            if vl < min_loss:\n",
    "                min_loss = vl\n",
    "                best_params = (p, b)\n",
    "\n",
    "    print(best_params)\n",
    "    (best_p, best_b) = best_params\n",
    "    \n",
    "    # plot our prediction on the training and validation set\n",
    "    tl, vl = main(x_train_val, best_p/100, best_b, plot_data=True)\n",
    "    print(vl)\n",
    "\n",
    "    # plot our prediction on the training+validation and testing set\n",
    "    val_cut = round(best_p/100 * test_cut)\n",
    "    x_train = x_train_val[:val_cut]\n",
    "    n_predict = length - val_cut\n",
    "    extrapolation = fourierExtrapolation(x_train, n_predict, best_b)\n",
    "    plot(x_train_val, x_test, extrapolation, sigma=50)\n",
    "    test_loss = mape(x_test, extrapolation[test_cut:])\n",
    "    print(test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial time series\n",
    "x = pm10_series.to_numpy()\n",
    "y = pm25_series.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur the ts\n",
    "x = gaussian_filter(x, sigma=50)\n",
    "y = gaussian_filter(y, sigma=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the ts to intervals\n",
    "x = x[2000:3000]\n",
    "y = y[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_distance, warp_path = fastdtw(x, y, dist=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Remove the border and axes ticks\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "#for [map_x, map_y] in warp_path[::10]:\n",
    "#    ax.plot([map_x, map_y], [x[map_x], y[map_y]], '-k')\n",
    "\n",
    "ax.plot(x, color='blue', linewidth=1)\n",
    "ax.plot(y, color='red', linewidth=1)\n",
    "#ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "\n",
    "#fig.savefig(\"plots/comparison_with_div_inter.png\", **savefig_options)"
   ]
  }
 ]
}