{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python379jvsc74a57bd0477d2b383c49e1ec6e86d0e875214c11d49545137877c5c3b99ad7c21c9bd5e4",
   "display_name": "Python 3.7.9 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thingspeak\n",
    "import pandas as pd\n",
    "import json\n",
    "import ssl\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import numpy as np\n",
    "import plotly.io as pio\n",
    "import datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import math\n",
    "import seaborn as sn\n",
    "from typing import Tuple\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastdtw import fastdtw\n",
    "from scipy.spatial.distance import euclidean\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "import scipy.stats as stats\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "\n",
    "# Configuring Matplotlib\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "savefig_options = dict(format=\"png\", dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "from scipy.ndimage.filters import gaussian_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = \"ZRDS32VNQEEFSOF4\"\n",
    "CHANNEL_ID = \"1361623\"\n",
    "\n",
    "fields = {\n",
    "    \"field2\": \"TEMP [C]\", \n",
    "    \"field3\":\"Relative humidity\", \n",
    "    \"field4\":\"PM1 [ug/m3]\", \n",
    "    \"field5\":\"PM2.5 [ug/m3]\", \n",
    "    \"field6\":\"PM10 [ug/m3]\",\n",
    "    \"created_at\" : \"Date/time\"\n",
    "}\n",
    "\n",
    "new_fields = {v : k for (k,v) in zip(fields.keys(), fields.values())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_plotly():\n",
    "    pd.options.plotting.backend = 'plotly'\n",
    "\n",
    "setup_plotly()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_thingspeak_data(id: str, api_key: str, nb_of_results: int = 8000, drop_entry_id: bool = True, save: bool = False) -> pd.DataFrame:\n",
    "    time_zone = \"UTC\"\n",
    "    #url = f\"https://thingspeak.com/channels/{id}/feed.csv?apikey={api_key}&results={nb_of_results}&timezone={time_zone}\"\n",
    "    \n",
    "    #df = pd.read_csv(url)\n",
    "    df = pd.read_csv('data/thingspeak_data.csv')\n",
    "\n",
    "    df.replace(['None'], np.nan, inplace=True)\n",
    "    df.rename(columns=fields, inplace=True)\n",
    "    df[\"Date/time\"] = pd.to_datetime(df[\"Date/time\"])\n",
    "    df[\"TEMP [C]\"] = df[\"TEMP [C]\"].astype(\"float32\")\n",
    "    df.set_index(\"Date/time\", inplace=True)\n",
    "    df.index = df.index.tz_convert(\"Europe/Paris\")\n",
    "    if save:\n",
    "        df.to_csv('data/thingspeak_data.csv')\n",
    "    df = df.assign(missing= np.nan)\n",
    "    df.drop(\"missing\", inplace=True, axis=1)\n",
    "    df.drop(\"Relative humidity\", axis=1, inplace=True)\n",
    "    df.drop(\"PM1 [ug/m3]\", axis=1, inplace=True)\n",
    "    \n",
    "    if drop_entry_id:\n",
    "        df.drop('entry_id', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zue_data():\n",
    "    df = pd.read_csv('data/ZUE.csv', delimiter=\";\")\n",
    "    df[\"Date/time\"] = pd.to_datetime(df[\"Date/time\"], format='%d.%m.%Y %H:%M')\n",
    "    df[\"TEMP [C]\"] = df[\"TEMP [C]\"].astype(\"float32\")\n",
    "    df.set_index(\"Date/time\", inplace=True)\n",
    "    df.index = df.index.tz_localize(\"Europe/Paris\", ambiguous=\"NaT\", nonexistent=\"shift_backward\")\n",
    "    df = df.assign(missing= np.nan)\n",
    "    df.drop(\"missing\", inplace=True, axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_series(df: pd.DataFrame):\n",
    "    pm10_series = df.loc[:, \"PM10 [ug/m3]\"]\n",
    "    pm25_series = df.loc[:, \"PM2.5 [ug/m3]\"]\n",
    "    temperature_series = df.loc[:, \"TEMP [C]\"]\n",
    "    precipitation_series = df.loc[:, \"PREC [mm]\"]\n",
    "\n",
    "    return temperature_series, precipitation_series, pm25_series, pm10_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we use a sliding window with std to detect and remove outliers\n",
    "def remove_outliers(df, param=1.6, hours=0, minutes=0):\n",
    "    for t in df.index:\n",
    "        t1 = t - timedelta(hours=hours, minutes=minutes)\n",
    "        t2 = t + timedelta(hours=hours, minutes=minutes)\n",
    "        inter = df.loc[t1:t2]\n",
    "        mean = inter.mean(axis=0)\n",
    "        std = inter.std(axis=0)\n",
    "        dist = np.abs(df.loc[t] - mean)\n",
    "        df.loc[t] = df.loc[t].where(dist < param * std)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we wanted to interpolate the large missing intervals with DTW but the results were inconclusive\n",
    "# due to the high correlation between pm10 and pm25 (~0.95) we decided to interpolate with a simple multiplication\n",
    "def interpolate_pm10(df):\n",
    "    pm10_series = df[\"PM10 [ug/m3]\"]\n",
    "    pm25_series = df[\"PM2.5 [ug/m3]\"]\n",
    "\n",
    "    mean_factor = (pm10_series/pm25_series).mean(axis=0)\n",
    "\n",
    "    # interpolate with multiplication of pm2.5\n",
    "    pm10_na = pm10_series.isna()\n",
    "    pm10_series[pm10_na] = mean_factor * pm25_series[pm10_na]\n",
    "\n",
    "    df[\"PM10 [ug/m3]\"] =  pm10_series\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def merge_datasets(df_zue: pd.DataFrame, df_ts: pd.DataFrame, save_data=False) -> pd.DataFrame:\n",
    "\n",
    "    # time interpolation corresponds to a linear interpolation but \n",
    "    # it takes into account the time distance between each index when\n",
    "    # assigning a value\n",
    "\n",
    "    # except for the case of pm10 we decided to use time interpolation for \n",
    "    # scattered missing values\n",
    "\n",
    "    # reorder the columns according to zue data\n",
    "    # crop the ts data that has no corresponding zue data\n",
    "    # resample the ts data to have exactly 5 minutes indexes\n",
    "    ts = df_ts.copy()\n",
    "    ts = ts[ts.index.notnull()]\n",
    "    ts = ts[df_zue.columns[:3]]\n",
    "    ts = ts.loc[:df_zue.index[-1]] \n",
    "    ts = ts.resample('5min').mean()\n",
    "    start = ts.index[0]\n",
    "\n",
    "    if save_data : ts.to_csv('data_v/ts.csv')\n",
    " \n",
    "    # crop zue data and save precipitations separately\n",
    "    zue = df_zue.copy()\n",
    "    zue = zue[zue.index.notnull()]\n",
    "    zue = zue.loc[:ts.index[-1]]\n",
    "    precipitations = zue[\"PREC [mm]\"]\n",
    "    zue.drop(\"PREC [mm]\", axis=1, inplace=True)\n",
    "\n",
    "    if save_data : zue.to_csv('data_v/zue.csv')\n",
    "\n",
    "    # interpolate the pm10 zue data for which we have large missing intervals\n",
    "    zue = interpolate_pm10(zue)\n",
    "\n",
    "    if save_data : zue.to_csv('data_v/zue_pm10_inter.csv')\n",
    "\n",
    "    # remove the outliers and interpolate their value with time\n",
    "    zue = remove_outliers(zue, hours=5)\n",
    "    ts = remove_outliers(ts, minutes=30)\n",
    "\n",
    "    zue = zue.interpolate(method='time')\n",
    "    ts = ts.interpolate(method='time')\n",
    "\n",
    "    if save_data : zue.to_csv('data_v/zue_no_outliers.csv')\n",
    "    if save_data : ts.to_csv('data_v/ts_no_outliers.csv')\n",
    "   \n",
    "    # upsample zue data to 5mn and interpolate with time\n",
    "    # separate between previous and overlapping data\n",
    "    zue = zue.resample('5min').interpolate('time')\n",
    "    zue_prev = zue.loc[:start].drop(start)\n",
    "    zue_superpos = zue.loc[start:ts.index[-1]]\n",
    "\n",
    "    if save_data : zue_superpos.to_csv('data_v/zue_superpos.csv')\n",
    "\n",
    "    # scale our ts data according to the zue data\n",
    "    h_mean_ts = pd.DataFrame().reindex_like(ts)\n",
    "    for t in ts.index:\n",
    "        t1 = t - timedelta(minutes=30)\n",
    "        t2 = t + timedelta(minutes=30)\n",
    "        h_mean_ts.loc[t] = ts.loc[t1:t2].mean(axis=0)\n",
    "\n",
    "    if save_data : h_mean_ts.to_csv('data_v/h_mean_ts.csv')\n",
    "\n",
    "    scaled_ts = zue_superpos * ts / h_mean_ts\n",
    "\n",
    "    if save_data : scaled_ts.to_csv('data_v/scaled_ts.csv')\n",
    "    \n",
    "    # calculate our scaled ts data variation around the zue data\n",
    "    dist_ts = scaled_ts - zue_superpos\n",
    "\n",
    "    #h = dist_ts.hist()\n",
    "    #h.show()\n",
    "    \n",
    "    # calculate our variation properties (assumed normal from the histogram)\n",
    "    means = dist_ts.mean(axis = 0)\n",
    "    covs = dist_ts.cov()\n",
    "\n",
    "    # add noise to the zue data with a distribution corresponding to the ts data\n",
    "    zue_prev += np.random.multivariate_normal(means, covs, zue_prev.shape[0])\n",
    "\n",
    "    # create the result dataset by concatenating the noisy previous zue data with\n",
    "    # the scaled ts data, adding back precipitations, interpolating any possibly missing values and \n",
    "    # setting the minimal value for the particule measures to 0\n",
    "    result = pd.concat([zue_prev, scaled_ts])\n",
    "    result[\"PREC [mm]\"] = precipitations\n",
    "    result = result.interpolate('time')\n",
    "    result[[\"PM10 [ug/m3]\", \"PM2.5 [ug/m3]\"]] = result[[\"PM10 [ug/m3]\", \"PM2.5 [ug/m3]\"]].clip(lower=0)\n",
    "\n",
    "    if save_data : result.to_csv('data_v/dataset.csv')\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(save: bool = True) -> pd.DataFrame: \n",
    "    df_zue = get_zue_data()\n",
    "    df_ts = get_thingspeak_data(id=CHANNEL_ID, api_key=API_KEY, save=True) \n",
    "    df = merge_datasets(df_ts=df_ts, df_zue=df_zue)\n",
    "    if save :\n",
    "        result.to_csv('data/dataset.csv')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "ipykernel_launcher:11: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n"
     ]
    }
   ],
   "source": [
    "df = get_dataset(save=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(array([  18,   18,   18,  122,  122,  122,  144,  144,  144,  174,  174,\n        174,  349,  351,  355,  360,  361,  362,  364,  366,  375,  376,\n        552,  552,  552,  634,  634,  634,  750,  750,  750,  860,  860,\n        860,  861,  861,  861,  862,  862,  862,  863,  863,  863, 1400,\n       1400, 1400, 1521, 1521, 1521, 1588, 1588, 1588], dtype=int64), array([0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0, 1, 2, 0,\n       1, 2, 0, 1, 2, 0, 1, 2], dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "df_ts = get_thingspeak_data(id=CHANNEL_ID, api_key=API_KEY, save=True)\n",
    "df_ts.index = df_ts.index.round('5T')\n",
    "df_ts = df_ts.groupby(level=0).mean().resample('5min').mean()\n",
    "df_ts2 = df_ts.resample('5min').mean()\n",
    "print(np.where(df_ts != df_ts2))\n",
    "df.plot(title=\"Comparison graph\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv('data_v/ts_no_outliers.csv')\n",
    "df1.set_index(\"Date/time\", inplace=True)\n",
    "df1.columns = df1.columns + ' TS'\n",
    "\n",
    "df2 = pd.read_csv('data_v/zue_superpos.csv')\n",
    "df2.set_index(\"Date/time\", inplace=True)\n",
    "df2.columns = df2.columns + ' Zue'\n",
    "\n",
    "df3 = pd.read_csv('data_v/h_mean_ts.csv')\n",
    "df3.set_index(\"Date/time\", inplace=True)\n",
    "df3.columns = df3.columns + ' TS Mean'\n",
    "\n",
    "df4 = pd.read_csv('data_v/scaled_ts.csv')\n",
    "df4.set_index(\"Date/time\", inplace=True)\n",
    "df4.columns = df4.columns + ' TS Scaled'\n",
    "\n",
    "dfplot = df1.append(df2).append(df3).append(df4)\n",
    "\n",
    "dfplot.plot(title=\"Comparison graph\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfplot = pd.read_csv('data_v/dataset.csv')\n",
    "dfplot.set_index(\"Date/time\", inplace=True)\n",
    "dfplot = dfplot.applymap(lambda x : max(x, 0))\n",
    "dfplot.plot(title=\"Comparison graph\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_series, precipitation_series, pm25_series, pm10_series = extract_time_series(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = df.plot(title='Dataset')\n",
    "fig.update_layout(width=1100)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrMatrix = df.corr()\n",
    "px.imshow(corrMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = sn.heatmap(corrMatrix, annot=True)\n",
    "plt.get_figure().savefig('plots/correlogram.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precipitation_series.plot(title=\"Precipitation time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temperature_series.plot(title=\"Temperature time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm10_series.plot(title=\"PM10 time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pm25_series.plot(title=\"PM2.5 time-series\", width=1100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial time series\n",
    "x = pm10_series.to_numpy()\n",
    "y = pm25_series.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#blur the ts\n",
    "x = gaussian_filter(x, sigma=7)\n",
    "y = gaussian_filter(y, sigma=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reduce the ts to intervals\n",
    "x = x[2000:3000]\n",
    "y = y[2000:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtw_distance, warp_path = fastdtw(x, y, dist=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(16, 12))\n",
    "\n",
    "# Remove the border and axes ticks\n",
    "fig.patch.set_visible(False)\n",
    "ax.axis('off')\n",
    "\n",
    "#for [map_x, map_y] in warp_path[::10]:\n",
    "#    ax.plot([map_x, map_y], [x[map_x], y[map_y]], '-k')\n",
    "\n",
    "ax.plot(x, color='blue', linewidth=1)\n",
    "ax.plot(y, color='red', linewidth=1)\n",
    "#ax.tick_params(axis=\"both\", which=\"major\", labelsize=18)\n",
    "\n",
    "fig.savefig(\"plots/comparison_with_div_inter.png\", **savefig_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_imputation_methods(df: pd.DataFrame)\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_percent_NAs(df: pd.DataFrame):\n",
    "    nans = pd.DataFrame(df.isnull().sum().sort_values(ascending=False)/len(df), columns=['percent']) \n",
    "    idx = nans['percent'] > 0\n",
    "    return nans[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_percent_NAs(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_train_test_validation_sets(X: np.array) -> Tuple[np.array, np.array, np.array]:\n",
    "    train_size = int(0.7 * len(X))\n",
    "    validation_size = int(0.1 * len(X))\n",
    "    test_size = len(X) - validation_size - train_size\n",
    "\n",
    "    train_set, test_set, validation_set = X[0:train_size, :], X[train_size: train_size + test_size,:], X[train_size + test_size: train_size + test_size + validation_size, :]\n",
    "    \n",
    "    print(f\"\"\"\n",
    "    Total dataset length: {len(X)}\n",
    "    \n",
    "    Train set shape: {train_set.shape} ({train_set.shape[0] / len(X) * 100:0.2f}% of dataset)\n",
    "    Test set shape: {test_set.shape} ({test_set.shape[0] / len(X) * 100:0.2f}% of dataset)\n",
    "    Validation set shape: {validation_set.shape} ({validation_set.shape[0] / len(X) * 100:0.2f}% of dataset)\n",
    "    \"\"\")\n",
    "\n",
    "    return train_set, test_set, validation_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dataset(df: pd.DataFrame) -> np.array:\n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    return scaler.fit_transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data = normalize_dataset(df=df)\n",
    "train_set, test_set, validation_set = split_into_train_test_validation_sets(X=scaled_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}